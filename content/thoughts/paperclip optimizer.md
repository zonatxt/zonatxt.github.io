---
title: "Paperclip optimizers"
date: 2022-04-12
tags:
  - seed
---

On [LessWrong](https://www.lesswrong.com/tag/paperclip-maximizer)

The paperclip maximizer is the canonical thought experiment showing how an [[posts/agi|artificial general intelligence]], even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an [existential threat](https://www.lesswrong.com/tag/existential-risk) due to over-optimization.
